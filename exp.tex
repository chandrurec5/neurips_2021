\section{Numerical Experiments}\label{sec:exp} 
 
% Prior vs now what is the difference : robustness + random label etc
%Datasets are MNIST and CIFAR-10. For MNIST, we use the DGN in \Cref{fig:dgn-prior-new} (right) with fully connected layers instead of the convolutional layers.  All models are trained with `\emph{Adam}'  \cite{adam} (step-size $=3\cdot 10^{-4}$ , batch size $=32$). 

\subsection{Experiment 1: DGN with 4 Gating Regimes and 48 Models}
We make use of the deep gated network (DGN) setup shown in \Cref{fig:dgn-prior-new} (right) which is an improvisation of the DGN in prior work [\citenum{npk}] shown in \Cref{fig:dgn-prior-new} (left). 

\textbf{Prior Work (4 Regimes, 1 Model).} 
Recall from \Cref{sec:dgn}, that a DGN consists of two distinct networks of identical architecture namely the feature network and the value network. Both the feature network and value network have $4$ convolutional layers $C1,\ldots,C4$ followed by a global average pooling (GAP) layer, a linear layer and a final `softmax' layer. $\hat{y}_{\text{DGN}}$ is the output of the DGN (performance is measured with respect to this output). $\hat{y}_{\text{f}}$ is used only when we want to \emph{pre-train} the feature network. The feature network has ReLUs, whereas the value network uses the gates from feature network as external masks. Here, $G_1,\ldots,G_4$ are the gates of layers $1,\ldots,4$ of feature network and in the prior setup these are used directly in the value network. The DGN has the following $\mathbf{4}$ {gating regimes} based on the initialisation and trainability of the feature network parameters. In all these regimes, the output of the DGN is $\hat{y}_{\text{DGN}}$. 

\quad 1. \emph{Fixed Random-Dependent Initialisation} (\textbf{FR-DI}): Both value network and feature network are initialised at random and identically, i.e., $\Tf_0=\Tv_0$. Only the value network is trained and the feature network is fixed, i.e., $\Tf_t=\Tf_0,\forall t\geq 0$.

\quad 2. \emph{Fixed Random-Independent Initialisation} (\textbf{FR-II}): Both value network and feature network are initialised at random and statistically independent of each other, i.e., $\Tf_0\perp \Tv_0$. Only the value network is trained and the feature network is fixed, i.e., $\Tf_t=\Tf_0,\forall t\geq 0$.

\quad 3. \emph{Fixed Learnt} (\textbf{FL}): The feature network is \textbf{pre-trained} first using $\hat{y}_{\text{f}}$ as output. Then the value network is initialised at random, and only the value network is trained and the feature network is fixed.

\quad 4. \emph{Decoupled learning} (\textbf{DL}): Both value and feature networks are initialised at random, and \textbf{both are trained}. In order to ensure that gradient flows through the feature network, we make use of \emph{soft-gating}, i.e., $G(q)=\frac{1}{1+\exp{-\beta\cdot q}}$, with $\beta=10$. 

\begin{figure}[h]
\centering
\begin{minipage}{0.49\columnwidth}
\centering
\resizebox{\columnwidth}{!}{
\includegraphics[scale=0.3]{figs/exp-prior.pdf}
}
\end{minipage}
\begin{minipage}{0.49\columnwidth}
\centering
\resizebox{\columnwidth}{!}{
\includegraphics[scale=0.3]{figs/exp-new.pdf}
}
\end{minipage}
\caption{The prior setup in [\citenum{npk}] is shown in the left and the setup in this paper is shown in the right. Here $C1,\ldots,C4$ are convolutional layers with $128$ output filters, kernel size $3\times 3$ and stride $1\times 1$.}
\label{fig:dgn-prior-new}
\end{figure}

\textbf{This Paper (4 Regimes, 48 Models ).} The improvisations are marked in {\color{red}{red}}.  In the current setup, {\bf{$G_{i_1},\ldots,G_{i_4}$ is a permutation of the $G_1,\ldots,G_4$}}, this gives $24$ models. In the prior setup both value and feature networks have the same input $x\in\R^{\din}$. In the current setup, we have two separate inputs, $x^{\text{f}}\in\R^{\din}$ for the feature network and $x^{\text{v}}\in\R^{\din}$ for the value network. We set $x^{\text{f}}=x$ always, however, for $x^{\text{v}}$ there are \textbf{two modes} namely (i) \textbf{standard}: we set $x^{\text{v}}=x$ ,(ii) \textbf{constant}: we set $x^{\text{v}}=\mathbf{1}\in\R^{\din}$ (a tensor of all $1$'s). While the setup on the left is only one model, the setup on the right has $\mathbf{48=24\times 2}$ \textbf{different models}, where $24=\texttt{factorial}(4)$ is due to the gate permutations and $2$ is due to the two different ways of setting $x^{\text{v}}$.






\begin{tabular}{|p{1.75cm}|c|c|c|c|c|}\hline
& FR-II &FR-DI & DL& FL& ReLU\\\hline
\centering{FC (MNIST)} &$94.1\%$  &$94.1\%$  &$98.1\%$ &$98.6\%$ &$98.5\%$\\\hline
\centering{CNN (CIFAR-10)}&$67.5\%$ &$67.6\%$   &$77.6\%$ &$79.4\%$ &$80.4\%$\\\hline
\end{tabular}


\textbf{Discussion.}

\indent\quad $1.$ \textbf{Decoupling gates and weights does not hurt.} There is no performance difference between FR-II and FR-DI.  Further, decoupled learning of gates (DL) performs significantly better than fixed random gates (FR), and the gap between standard DNN with ReLU and DL is less than $3\%$. This marginal performance loss seems to be worthy trade off for fundamental insights of \Cref{th:main} under the decoupling assumption.

\indent\quad $2.$ \textbf{Features are in the gates.} The fixed learnt regime (FL) shows that using the gates of a pre-trained ReLU network, performance can be recovered by training the NPV. Also, by interpreting the input dependent component of a model to be the features and the input independent component to be the weights, it makes sense to look at the gates/NPFs as the hidden features and NPV as the weights.% (which can be re-trained).

\indent\quad $3.$ \textbf{Random gates perform well.} FR-II does perform well in all the experiments (note that for a $10$-class problem, a random classifier would achieve only $10\%$ test accuracy). Given the observation that the gates are the true features, and the fact that is there no learning in the gates in the fixed regime, and the performance of fixed random gates can be purely attributed to the in-built structure.

\indent\quad $4.$ \textbf{Gate Learning explain finite vs infinite width.} We group the models into three sets where $S_1=\{$ ReLU, FL , DL$\}$, $S_2=\{$ FR$\}$ and $S_3=\{$ CNTK $\}$, and explain the difference in performance due to gate learning.
 $S_2$ and $S_3$ have no gate learning. However,  $S_3$ due to its infinite width has better averaging resulting in a well formed kernel and hence performs better than $S_2$ which is a finite width. Thus, the difference between $S_2$ and $S_3$ can be attributed to finite versus infinite width. Both $S_1$ and $S_2$ are finite width, and hence, conventional feature learning happens in both $S_1$ and $S_2$, but, $S_1$ with gate learning is better ($77.5\%$ or above in CIFAR-10) than $S_2$ ($67\%$ in CIFAR-10) with no gate learning. Thus neither finite width, nor the conventional feature learning explain the difference between $S_1$ and $S_2$. Thus, `gate learning' discriminates the regimes $S_1, S_2$ and $S_3$ better than the conventional feature learning view.

\indent\quad $5.$ \textbf{Robustness to permutation:} The performance (in all the $4$ regimes) is also robust to permutation of layers. This can be attributed to the product $\Pi_{l=1}^{(d-1)} H^{\text{lyr}}_{l,\Theta}$ of the layer level base kernels being permutation invariant. A particularly striking example of robustness to permutations and constant input is presented in \Cref{fig:visual-permute}. Here,  we contrast the hidden layer outputs of a standard CNN with ReLU with $4$ layers, and that of a DGN which copies the gates from the standard CNN, but, reverses the gating masks when applying to the value network. Also, the value network of the DGN was provided with a fixed random input. Both the models achieve about $80\%$ test accuracy.  This suggests that visually interpreting the hidden layer outputs may not be meaningful.

\indent\quad $6.$ \textbf{Robustness to constant input:} The performance (in all the $4$ regimes) is  robust to `all-ones' inputs. Note that in the `all-ones' case, the input information affects the models only via the gates. Here, all the entries of the input Gram matrix are identical, and the NPK depends only on the base kernels.


\subsection{Experiment 2: Upstream training with random labels and downstream with true labels}


\textbf{Q1 (open question).} {When trained with random labels upstream followed by true labels downstream, the test performance of DNNs with ReLUs degrades, Why?}

We hypothesise that the answer to the above question lies in the gates. To confirm our hypothesis, we train in two phases (i) Phase I: upstream training with label noise levels $\gamma=0, 25\%, 50\%, 75\%$ and (ii) Phase II: downstream training with true labels. We then measure the information stored in the gates at the end of each of the two phases. To this end, we consider the DGN setup in \Cref{fig:dgn-prior-new} (left), and train the feature network (which is a DNN with ReLUs). First is `Phase I: Upstream', wherein, we train the feature network for different values of label noise $\gamma=0, 25\%, 50\%, 75\%$, which gives us models M1$(\gamma)$. In order to measure the information in the gates learnt at the end of `Phase I: Upstream', we keep these gates fixed and train the value network with true labels -- this gives us models M2($\gamma$). Second is `Phase II: Downstream', wherein, we start with models M1$(\gamma)$, and perform downstream training with true labels to obtain models M4$(\gamma)$ ($\gamma=0$ is an exception; there is no downstream training because it has been already trained with true labels in upstream).

\textbf{Q2 (new question).} {When trained with random labels upstream followed by true labels downstream, \emph{if the gates are fixed throughout and only the weights are trained}, does the test performance degrade?}


We are interested in measuring the information stored in the gates of a DNN with ReLUs trained with random labels. We are also interested in testing the hypothesis $\mathcal{H}$ : upstream training with random labels followed by downstream training with true labels only affects the gates which is also the reason for the degradation in downstream performance. This hypothesis also means hypothesis $\mathcal{H'}$ : if the gates are fixed then upstream training with random labels followed by downstream training with true labels should not degrade the performance. To this end we do the following

\indent \emph{Step 1}. We first train models (DNNs with ReLUs namely) $M1, M4(25)$ and $M4(50)$ with no label noise (i.e., true lables), $\gamma=25\%$ label noise and $\gamma =50\%$ label noise respectively.

\indent \emph{Step 2}. In order to measure the information stored in the gates, we use the gates of pre-trained models $M1, M4(25)$ and $M4(50)$ as external masks in value networks $M2$, $M5(25)$ and $M5(50)$, and train the value networks $M2$, $M4(25)$ and $M(50)$ using dataset with true labels.

\indent \emph{Step 3}. In order to measure the robustness of the gates, we use the gates of pre-trained models $M1, M4(25)$ and $M4(50)$ as external masks in value networks $M3$, $M6(25)$ and $M6(50)$. We first perform downstream training of the value networks $M3$, $M6(25)$ and $M6(50)$ with a dataset containing $100\%$ label noise and then we perform upstream training of these models with dataset containing all true labels.

\indent \emph{Step 4.} We then perform downstream training of models $M4(25)$ and $M4(50)$ with true labels to yield models $M7(25)$ and $M7(50)$ respectively. We then measure the information in the gates of $M7(25)$ and $M7(50)$ by using their gates as external masks in value networks $M8(25)$ and $M8(50)$ and training them with true labels.

The entire process can be thought of as two phases namely  `Phase I', which is upstream training with random labels followed by `Phase II' which is downstream training with true labels. In both `Phase I and II', we measure the information stored in the gates under two cases (i) \emph{FLG 1:} training the value network with true labels and (ii) \emph{FLG 2:} training the value network with first $100\%$ label noise followed by true labels. `FLG 2' is to verify the hypothesis $\mathcal{H'}$. In the case of training with no label noise, there is only a single phase, in which we have the two cases.

\begin{figure}
\centering
\includegraphics[scale=0.3]{figs/rand-label.pdf}
\end{figure}




\begin{table}[h]
\begin{tabularx}{\columnwidth}{c *{6}{Y}}
\toprule

 & \multicolumn{4}{c}{Phase I: Upstream}  
 & \multicolumn{2}{c}{Phase II: Downstream}\\
\cmidrule(lr){2-5} \cmidrule(l){6-7}
&  \multicolumn{2}{c}{ReLU}  &\multicolumn{2}{c}{Fixed Gates} & ReLU & {Fixed Gates}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7}
$\gamma$& Best & End & True& US/DS & True & True\\\hline\arrayrulecolor{white}\midrule
0 &{\bf{81.2}}{\tiny $\pm$ 0.3} & 80.0{\tiny $\pm$ 0.4} &{\bf{80.3}}{\tiny $\pm$ 0.2} & {\bf{79.3}}{\tiny $\pm$ 0.4} &-&- \\\hline\hline
{25}&76.1{\tiny $\pm$ 0.6} & 63.1{\tiny $\pm$ 0.7}& 74.7{\tiny $\pm$ 0.5}& 72.3{\tiny $\pm$ 0.2}&76.9{\tiny $\pm$ 0.1}&76.9{\tiny $\pm$ 0.4}\\\hline\hline
{50}&70.9{\tiny $\pm$ 0.8} & 41.5{\tiny $\pm$ 1.1}& 69.9{\tiny $\pm$ 0.3} & 66.8{\tiny $\pm$ 0.1}&73.4{\tiny $\pm$ 0.3}&73.6{\tiny $\pm$ 0.4}\\\hline\hline
{75}&58.2{\tiny $\pm$ 1.0} & 19.5{\tiny $\pm$ 6.0}& 64.6{\tiny $\pm$ 0.9} &60.8{\tiny $\pm$ 0.3}&69.9{\tiny $\pm$ 0.9}&68.9{\tiny $\pm$ 0.9}\\\hline
\arrayrulecolor{black}\bottomrule
Models & \multicolumn{2}{c}{M1($\gamma$)} & M2($\gamma$) & M3($\gamma$)& M4($\gamma$) & M5($\gamma$)\\\bottomrule
\end{tabularx}

\end{table}





\begin{figure}[h]
\begin{comment}
\begin{minipage}{0.4\columnwidth}
\resizebox{\columnwidth}{!}{
\includegraphics[scale=0.35]{figs/rand-label.pdf}
}
\end{minipage}
\end{comment}
\begin{minipage}{0.99\columnwidth}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccc}
\includegraphics[scale=0.125]{figs/relu_25.pdf}&
\includegraphics[scale=0.125]{figs/galu_25_good.pdf}&
\includegraphics[scale=0.125]{figs/galu_25_bad.pdf}&
\includegraphics[scale=0.125]{figs/galu_25_bad_good.pdf}&
\includegraphics[scale=0.125]{figs/relu_25_good.pdf}&
\includegraphics[scale=0.125]{figs/galu_25_recovered.pdf}
\\
\includegraphics[scale=0.125]{figs/relu_50.pdf}&
\includegraphics[scale=0.125]{figs/galu_50_good.pdf}&
\includegraphics[scale=0.125]{figs/galu_50_bad.pdf}&
\includegraphics[scale=0.125]{figs/galu_50_bad_good.pdf}&
\includegraphics[scale=0.125]{figs/relu_50_good.pdf}&
\includegraphics[scale=0.125]{figs/galu_50_recovered.pdf}
\\
\includegraphics[scale=0.125]{figs/relu_75.pdf}&
\includegraphics[scale=0.125]{figs/galu_75_good.pdf}&
\includegraphics[scale=0.125]{figs/galu_75_bad.pdf}&
\includegraphics[scale=0.125]{figs/galu_75_bad_good.pdf}&
\includegraphics[scale=0.125]{figs/relu_75_good.pdf}&
\includegraphics[scale=0.125]{figs/galu_75_recovered.pdf}\\
\tiny{M1}&\tiny{M2}&\tiny{M3:US}&\tiny{M3:DS}&\tiny{M4}&\tiny{M5}\\
\end{tabular}
}
\end{minipage}



\end{figure}

\begin{comment}
\begin{figure}[h]
\begin{minipage}{0.40\columnwidth}
\begin{tabular}{|c|c|c|}\hline
ReLU& \multicolumn{2}{c|}{}\\
with   & \multicolumn{2}{c|}{Fixed Learnt Gates from }\\
 True &\multicolumn{2}{c|}{ReLU with True Labels}\\
Labels &  \multicolumn{2}{c|}{}\\\cline{2-3}
&True & {US/DS}\\\hline
 80.8& 80& 79\\\hline
 col-1 & col-2 & col-3\\\hline
\end{tabular}

\end{minipage}
\begin{minipage}{0.62\columnwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
\%& \multicolumn{3}{c|}{\multirow{2}{*}{ReLU}} & \multicolumn{2}{c|}{FLG at End}&{FLG at End}\\
Label& \multicolumn{3}{c|}{{}} &\multicolumn{2}{c|}{of ReLU US}& of ReLU DS\\\cline{2-7}
Noise & \multicolumn{2}{c|}{Rand. US} & True & \multicolumn{1}{c|}{\multirow{2}{*}{True}} &{US/DS}& \multicolumn{1}{c|}{\multirow{2}{*}{True}} \\\cline{2-3}
{} & Best & End & DS &{} &  &\\\hline
25&75.3 & 63.1& 76.7&74.2 & 72.3& 76.5\\\hline
50& 69.8 &41.5&73.1&69.6 &67& 73.1\\\hline
 col-1 & col-2 & col-3&  col-4 & col-5 & col-6& col-7\\\hline
\end{tabular}
\end{minipage}
\end{figure}
\end{comment}




\subsection{Experiment 3: Adversarial Transferability via Gates}


\subsection{Experiment 4: Training only the gates}


\subsection{Experiment 5: Are Features Learnt Hierarchically? Yes and No.}

\input{visual}



\indent\quad $3.$ \emph{Gate Only Training:} In this setting, both value network and the feature network are initialised at random. However, only the feature network is trained and the value network weights are kept fixed. We observe about $65\%$ test accuracy on CIFAR-10 by just training the gates. This experiment can be interpreted as tuning the gates in such a manner to select the right sub-network of random weights.
 