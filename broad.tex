\section{Broader Impact}
This paper argues that the correlation of the \emph{on/off} states of the rectified linear units holds the key in understanding of inner workings of deep neural networks with rectified linear units. This paper also proposes a deep gated network in which the feature generation is carried out without using any activation functions, and is separate from the gates and the weights, thereby clearly demarcating the role of the different parts. We believe that this work will be a useful addition towards understanding deep neural networks and making them more interpretable and explainable. 