\section{Introduction}
Understanding the inner workings of deep neural networks (DNNs) is an important problem in machine learning. Recent works \citenum{ntk,arora2019exact,cao2019generalization} have connected the training and generalisation of DNNs to kernel methods. An important kernel associated with a DNN is its \emph{neural tangent kernel} (NTK), which, for a pair of input examples $x,x'\in\R^{\din}$, and network weights $\Theta\in\R^{\dnet}$, is given by:
\begin{align*}
 \text{NTK}(x,x')\quad = \quad \ip{\nabla_{\Theta}\hat{y}(x), \nabla_{\Theta}\hat{y}(x')}, 
\end{align*}
$\hat{y}_\Theta(\cdot)\in\R$ is the DNN output. It was shown that as the width of the DNN goes to infinity the NTK matrix converges to a limiting deterministic matrix $\text{NTK}_{\infty}$ and training an infinitely wide DNN is equivalent to a kernel method with $\text{NTK}_{\infty}$. While these recent results allows us to look at DNNs from the lens of kernels, there are some important issues: (i) \textbf{feature learning:} $\text{NTK}_{\infty}$ being a deterministic matrix does not capture feature learning whereas the success of DNNs is due to feature learning, (ii) \textbf{finite vs infinite width:} finite width convolutional neural network outperforms its corresponding $\text{NTK}_{\infty}$ and (iii)  \textbf{non-interprability:} the NTK is the inner product of gradients and has no physical interpretation. As a result, NTK theory does not fully explain the success of DNNs.

 \cite{npk} aimed at understanding DNNs with rectified linear units (ReLUs). Such DNNs have a special property in that each ReLU is also a gate which either blocks (multiplies by 0) or allows its pre-activation input (multiplies by 1). 
 Using the \emph{dual view} they characterised the role of gating analytically and empirically, showing that most useful information is learnt in the gates. The dual view also successfully addresses the issues of \textbf{feature learning, finite vs infinite width and non-interpretability} present in the NTK theory as described below.

$\bullet$ \textbf{Dual View:}  The standard primal way of expressing information processing is layer by layer.  In the dual view, gating property of ReLU is exploited to break the DNN into paths. A path comprises of gates and weights, and a path is `active' or `on' only if all the gates in the path are `on'. %The output is the sum of the contribution of the individual paths. 

$\bullet$ \textbf{Measure of information in the gates:} The gates are treated as masks and are decoupled from the weights by storing the gates and weights in two separate networks. The information in the gates is be measured by fixing the gates, training only the weights and looking at the test performance.  

$\bullet$ \textbf{Gates = Features:} When the gates from a trained DNN are used as masks, and the weights are trained from scratch, there is no significant loss in test performance, i.e., \textbf{features are in the gates}. Also, the performance of $\text{NTK}_{\infty}$ lies between the performance of untrained and trained gates, i.e., \textbf{learning in the gates explains the difference between finite vs infinite} width DNNs with ReLUs.

$\bullet$ \textbf{Interpretable Kernel:} Each input has a corresponding \emph{active} sub-network (comprising of the gates that are `on' and weights between such gates) which produces the output.  When the gates are decoupled from the weights, the NTK simplifies into a constant times a \emph{neural path kernel} (NPK):
\begin{align}\label{eq:ntk-npk-relation}
\text{NTK}(x,x')\quad\propto\quad \text{NPK}(x,x')\quad =\quad \ip{x,x'}\cdot {\bf{overlap}}(x,x'),
\end{align}
where ${\bf{overlap}}(x,x')$ is a correlation matrix that measure the overlap of active sub-networks in terms of the  total number of paths that are `active' for both inputs $x$ and $x'$. 

\begin{comment}
\begin{center}
\emph{Duality: DNNs with ReLU are layers as well as paths}
\end{center}
The standard primal way of expressing information processing is layer by layer. In the dual view, the DNN is broken into paths. A path comprises of gates and weights, and a path is `active' or `on' only if all the gates in the path are `on'. The output is the sum of the contribution of the individual paths. 
\begin{center}
\emph{Most information (i.e., features) is in the gates and gates are learnt during training}
\end{center}
The gates are treated as masks and are decoupled from the weights by storing the gates and weights in two separate networks (see \Cref{sec:dgn}). Now, the information in the gates can be measured by fixing the gates and training the weights. It was shown that (i) when the gates from a trained DNN are used as masks, and the weights are trained from scratch, there is no significant loss in test performance, i.e., \textbf{features are stored in the gates} and (ii) gates of a trained network perform better than $\text{NTK}_{\infty}$ and gates from a untrained network performs poorly than $\text{NTK}_{\infty}$, i.e., \textbf{learning in the gates explains the difference between finite vs infinite} width DNNs with ReLUs.
\begin{center}
\emph{NTK is interpretable in terms of active sub-networks}
\end{center}
\end{comment}


\subsection{Our Contribution}
In this paper, we present four results that strengthen the dual view and the importance of gates.  The first result is theoretical and is justified by the second result which is experimental. The third result settles an important open problem using the dual view showcasing its usefulness. The final result is empirical and builds on the insights obtained from the first three results. 

\indent\quad$1.$ \textbf{Product Kernel:} Expression in \eqref{eq:ntk-npk-relation} is in terms of the active sub-network overlap. We further simplify this down to the gates to show that the NPK has the following product structure:
\centerline{$\text{NTK}(x,x') \propto \langle x,x'\rangle\cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w,$}

where $G_l(x)\in\{0,1\}^w$ is the binary feature encoding the gates of layer $l$. The main takeaway are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$ measuring the \textbf{\emph{correlation of gates} }.

\indent\quad$2.$ \textbf{Robustness of Gates:} We show that operations such as permutation of layers, arbitrary tiling cum rotation of the gates and providing constant input, that destroy the \emph{layer-by-layer} structure do not degrade performance because the `correlation of gates' is invariant to these operations.

\indent\quad$3.$\textbf{Gates and Random Labels:} Upstream training with random labels followed by downstream training with true labels degrades test performance [\citenum{randlabel}]. We show that this degradation is due to the fact that random labels affect the gates, and gates do resist such degradation to a large extent. Also, if the gates are fixed, upstream training by random labels does not degrade test performance.

\indent\quad$4.$ \textbf{No Hidden Features:} We modify standard architectures (VGG19 and a ResNet) to yield two deep gated networks in which feature extractor is free of activations and is separate from the gates and the weights -- these achieve greater than $90\%$ test accuracy on CIFAR-10. Since the feature extractor uses only standard `image processing' operations, the features are interpretable and no longer hidden.

%\item Adversarial examples can attack the model solely via the linear component.

\textbf{Technical Extension.} We also extend the dual view to cover the cases of convolutions with pooling and skip connections. We show that convolutional layers with pooling makes the NPK \emph{rotationally invariant} and skip connections endow a \emph{sum of product of base kernels} structure to the NPK.
% (this property has already been noted to hold for the NTK). We also consider a residual network (ResNet) architecture with $(b+2)$ blocks of fully connected networks with `$b$' skip connections between them. Within this ResNet there are $i=1,\ldots,2^b$ possible fully connect networks, and then $\text{NPK}^{\text{residual}}=\sum_{i=1}^{2^b} C_i \text{NPK}^{\text{dense}}_i$, where $C_i>0$ are positive constants.
\begin{comment}

CNN, Resnet to say that the framework can be extended successfully.

Perhaps the second important and powerful insights lie in the experimental implication contribution

why do we even care for a specific theory?

All through we differentiate from \cite{npk}
\end{comment}