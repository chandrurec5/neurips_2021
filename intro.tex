\section{Introduction}
Understanding the inner workings of deep neural networks (DNNs) is an important problem in machine learning. Recent works \citenum{ntk,arora2019exact,cao2019generalization} have connected the training and generalisation of DNNs to kernel methods. An important kernel associated with a DNN is its \emph{neural tangent kernel} (NTK), which, for a pair of input examples $x,x'\in\R^{\din}$, and network weights $\Theta\in\R^{\dnet}$, is given by:
\begin{align*}
 \text{NTK}(x,x')\quad = \quad \ip{\nabla_{\Theta}\hat{y}(x), \nabla_{\Theta}\hat{y}(x')}, 
\end{align*}
$\hat{y}_\Theta(\cdot)\in\R$ is the DNN output. It was shown that as the width of the DNN goes to infinity the NTK matrix converges to a limiting deterministic matrix $\text{NTK}_{\infty}$ and training an infinitely wide DNN is equivalent to a kernel method with $\text{NTK}_{\infty}$. While these recent results allows us to look at DNNs from the lens of kernels, there are some important issues: (i) \textbf{feature learning:} $\text{NTK}_{\infty}$ being a deterministic matrix does not capture feature learning whereas the success of DNNs is due to feature learning, (ii) \textbf{finite vs infinite width:} finite width convolutional neural network outperforms its corresponding $\text{NTK}_{\infty}$ and (iii)  \textbf{non-interprability:} the NTK is the inner product of gradients and has no physical interpretation. As a result, NTK theory does not fully explain the success of DNNs.

 \cite{npk} aimed at understanding DNNs with rectified linear units (ReLUs). Such DNNs have a special property in that each ReLU is also a gate which either blocks (multiplies by 0) or allows its pre-activation input (multiplies by 1). 
 Using the \emph{dual view} they characterised the role of gating analytically and empirically, showing that most useful information is learnt in the gates. The dual view also successfully addresses the issues of \textbf{feature learning, finite vs infinite width and non-interpretability} present in the NTK theory as described below.

$\bullet$ \textbf{Dual View:}  The standard primal way of expressing information processing is layer by layer.  In the dual view, gating property of ReLU is exploited to break the DNN into paths. A path comprises of gates and weights, and a path is `active' or `on' only if all the gates in the path are `on'. %The output is the sum of the contribution of the individual paths. 

$\bullet$ \textbf{Measure of information in the gates:} The gates are treated as masks and are decoupled from the weights by storing the gates and weights in two separate networks. The information in the gates is be measured by fixing the gates, training only the weights and looking at the test performance.  

$\bullet$ \textbf{Gates = Features:} It was shown that when the gates from a trained DNN are used as masks, and the weights are trained from scratch, there is no significant loss in test performance, i.e., \textbf{features are stored in the gates}. Further, gates of a trained network perform better than $\text{NTK}_{\infty}$ and gates from a untrained network performs poorly than $\text{NTK}_{\infty}$, i.e., \textbf{learning in the gates explains the difference between finite vs infinite} width DNNs with ReLUs.

$\bullet$ \textbf{Interpretable Kernel:} Each input has a corresponding \emph{active} sub-network comprising of the gates that are `on' and weights through such gates. This active sub-network is responsible for producing the output for that input. When the gates are decoupled from the weights, the NTK simplifies into a constant times a \emph{neural path kernel} (NPK) given by :
\begin{align}\label{eq:ntk-npk-relation}
\text{NTK}(x,x')\quad\propto\quad \text{NPK}(x,x')\quad =\quad \ip{x,x'}\cdot {\bf{overlap}}(x,x'),
\end{align}
where ${\bf{overlap}}(x,x')$ is a correlation matrix that measure the overlap of active sub-networks in terms of the  total number of paths that are `active' for both inputs $x$ and $x'$. 

\begin{comment}
\begin{center}
\emph{Duality: DNNs with ReLU are layers as well as paths}
\end{center}
The standard primal way of expressing information processing is layer by layer. In the dual view, the DNN is broken into paths. A path comprises of gates and weights, and a path is `active' or `on' only if all the gates in the path are `on'. The output is the sum of the contribution of the individual paths. 
\begin{center}
\emph{Most information (i.e., features) is in the gates and gates are learnt during training}
\end{center}
The gates are treated as masks and are decoupled from the weights by storing the gates and weights in two separate networks (see \Cref{sec:dgn}). Now, the information in the gates can be measured by fixing the gates and training the weights. It was shown that (i) when the gates from a trained DNN are used as masks, and the weights are trained from scratch, there is no significant loss in test performance, i.e., \textbf{features are stored in the gates} and (ii) gates of a trained network perform better than $\text{NTK}_{\infty}$ and gates from a untrained network performs poorly than $\text{NTK}_{\infty}$, i.e., \textbf{learning in the gates explains the difference between finite vs infinite} width DNNs with ReLUs.
\begin{center}
\emph{NTK is interpretable in terms of active sub-networks}
\end{center}
\end{comment}


\subsection{Our Contribution}
\begin{comment}
Expression in \eqref{eq:ntk-npk-relation} is in terms of the active sub-network overlap. In this paper, we further simplify this down to the gates. We `re-write' \eqref{eq:ntk-npk-relation} (a simple step) in the form of a \emph{product kernel} given by:
\begin{align}\label{eq:main}
\text{NTK}(x,x') \propto \langle x,x'\rangle\cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w, 
 \end{align}
where $G_l(x)\in\{0,1\}^w$ is the binary feature encoding the gates of layer $l$. Product kernel in \eqref{eq:main} leads to the following interpretation: the basic building units are the gates which form the binary feature $G_l(x)$; each layer has an associated \emph{base kernel} given by $\frac{\ip{G_l(x),G_l(x')}}w$, where width gives averaging (division by $w$); laying out the gates as masks depth-wise and connecting them with weights in the form of network provides the product structure $\Pi_{l=1}^{d-1} (\cdot)$\footnote{Mere concatenation (instead of a network layout) of the gates as $\varphi(x)=(G_l(x),l=1,\ldots,d-1)\in\{0,1\}^{(d-1)w}$ gives the kernel $\ip{\varphi(x),\varphi(x')}=\sum_{l=1}^{d-1}\frac{\ip{G_l(x),G_l(x')}}w$, i.e., a sum  (not product) of base kernels.}. To our knowledge, \eqref{eq:main} is the simplest kernel expression in literature that analytically characterises `what' is learnt in a DNN with ReLUs, and explicitises roles of depth, width and gates. At the heart is the base kernel  $\frac{\ip{G_l(x),G_l(x')}}w$ which measures the \emph{correlation of gating activity} (in contrast to correlation of outputs as in Neural Gaussian Process kernel \cite{convgp,fcgp}).

Expression in \eqref{eq:ntk-npk-relation} is in terms of the active sub-network overlap. In this paper, we further simplify this down to the gates. We `re-write' \eqref{eq:ntk-npk-relation} (a simple step) in the form of a \emph{product kernel} given by:
\begin{align}\label{eq:main}
\text{NTK}(x,x') \propto \langle x,x'\rangle\cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w, 
 \end{align}
where $G_l(x)\in\{0,1\}^w$ is the binary feature encoding the gates of layer $l$. Product kernel in \eqref{eq:main} leads to the following interpretation: the basic building units are the gates; the gates of a layer `$l$' yield the binary feature $G_l(x)$; at the heart are the \textbf{base kernels  $\frac{\ip{G_l(x),G_l(x')}}w$ which measure the \emph{correlation of gates}} (in contrast to correlation of outputs as in Neural Gaussian Process kernel \citenum{convgp,fcgp}); width gives averaging (division by $w$); laying out the gates as masks depth-wise and connecting them with weights in the form of a network provides the product structure $\Pi_{l=1}^{d-1} (\cdot)$\footnote{Mere concatenation (instead of a network layout) of the gates as $\varphi(x)=(G_l(x),l=1,\ldots,d-1)\in\{0,1\}^{(d-1)w}$ gives the kernel $\ip{\varphi(x),\varphi(x')}=\sum_{l=1}^{d-1}\frac{\ip{G_l(x),G_l(x')}}w$, i.e., a \textbf{sum  (not product)} of base kernels.}. To our knowledge, \eqref{eq:main} is the simplest kernel expression in literature that analytically characterises `what' is learnt in a DNN with ReLUs, and explicitises roles of depth, width and gates. Expression \eqref{eq:main} suggests that `correlation of the gates' is key and any operation that does not disturb it should not degrade the performance. 
\end{comment}
Expression in \eqref{eq:ntk-npk-relation} is in terms of the active sub-network overlap. In this paper, we further simplify this down to the gates. We `re-write' \eqref{eq:ntk-npk-relation} (a simple step) in the form of a \emph{product kernel} given by:
\begin{align}\label{eq:main}
\text{NTK}(x,x') \propto \langle x,x'\rangle\cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w, 
 \end{align}
where $G_l(x)\in\{0,1\}^w$ is the binary feature encoding the gates of layer $l$. The main message is that \textbf{\emph{correlation of gates} $\ip{G_l(x),G_l(x')}$} characterises `what is learnt by DNNs with ReLUs'. We support our theory by the following experiments.
\begin{enumerate}
\item \textbf{Robustness of Gates:} We show that operations such as (i) permutation of layers, (ii) arbitrary tiling cum rotation of the gates, (iii) providing constant input, that destroy the \emph{layer-by-layer} information processing do not degrade performance because the `correlation of gates' is invariant to these operations.
\item \textbf{Gates and Random Labels:} It was recently reported \citenum{randlabel} that upstream training with random labels followed by downstream training with true labels degrades test performance. We show that this degradation is due to the fact that random labels affect the gates, and gates do resist such degradation to a large extent. Further, if the gates are fixed, upstream training by random labels does not significantly degrade test performance.
\item \textbf{An Interpretable Gated Network:} We build two deep gated networks in which feature extraction is fully linear and is separate from the gating and the weights -- these achieve $75\%$ and $90\%$ of CIFAR-10.

%\item Adversarial examples can attack the model solely via the linear component.
\end{enumerate}
\textbf{Technical Extension.} We also extend the dual view to cover the cases of convolutions with pooling and skip connections. We show that convolutional layers with pooling makes the NPK \emph{rotationally invariant} and skip connections endow a \emph{sum of product of base kernels} structure to the NPK.
% (this property has already been noted to hold for the NTK). We also consider a residual network (ResNet) architecture with $(b+2)$ blocks of fully connected networks with `$b$' skip connections between them. Within this ResNet there are $i=1,\ldots,2^b$ possible fully connect networks, and then $\text{NPK}^{\text{residual}}=\sum_{i=1}^{2^b} C_i \text{NPK}^{\text{dense}}_i$, where $C_i>0$ are positive constants.
\begin{comment}

CNN, Resnet to say that the framework can be extended successfully.

Perhaps the second important and powerful insights lie in the experimental implication contribution

why do we even care for a specific theory?

All through we differentiate from \cite{npk}
\end{comment}