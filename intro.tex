\section{Introduction}
Recent works have connected the training and generalisation of deep neural networks (DNNs) to kernel methods. An important kernel associated with a DNN is its \emph{neural tangent kernel} (NTK), which, for a pair of input examples $x,x'\in\R^{\din}$, and network weights $\Theta\in\R^{\dnet}$, is given by:
\begin{align*}
 \text{NTK}(x,x')\quad = \quad \ip{\nabla_{\Theta}\hat{y}(x), \nabla_{\Theta}\hat{y}(x')}, 
\end{align*}
$\hat{y}_\Theta(\cdot)\in\R$ is the DNN output. It was shown that as the width of the DNN goes to infinity the NTK matrix converges to a limiting deterministic matrix $\text{NTK}_{\infty}$ and training an infinitely wide DNN is equivalent to a kernel method with $\text{NTK}_{\infty}$. While these recent results allows us to look at DNNs from the lens of kernels, there are some important issues: (i) \textbf{feature learning:} $\text{NTK}_{\infty}$ being a deterministic matrix does not capture feature learning whereas the success of DNNs is due to feature learning, (ii) \textbf{finite vs infinite width:} finite width convolutional neural network outperforms its corresponding $\text{NTK}_{\infty}$ and (iii)  \textbf{non-interprability:} the NTK is the inner product of gradients and has no physical interpretation. As a result, NTK theory does not fully explain the success of DNNs.

\textbf{Duality for DNNs with Rectified Linear Units (ReLUs)}.  \cite{npk}, [\citenum{npk}] observed that each ReLU is also a gate which either blocks (multiplies by 0) or allows its pre-activation input (multiplies by 1) and showed that in a DNN with ReLUs most useful information is learnt its gates. To this end, they used \emph{dual view} to analytically and empirically characterise the role of gating. The dual view also successfully addresses the issues of \textbf{feature learning, finite vs infinite width and non-interpretability} present in the NTK theory as described below.

$\bullet$ \textbf{Dual View:}  The standard primal way of expressing information processing is layer by layer.  In the dual view, gating property of ReLU is exploited to break the DNN into paths. A path comprises of gates and weights, and a path is `active' or `on' only if all the gates in the path are `on'. The output is the sum of the contribution of the individual paths. 

$\bullet$ \textbf{Information in the gates:} The gates are treated as masks and are decoupled from the weights by storing the gates and weights in two separate networks (see \Cref{sec:dgn}). Now, the information in the gates is be measured by fixing the gates, training only the weights and looking at the test performance.  

$\bullet$ \textbf{Features:} It was shown that when the gates from a trained DNN are used as masks, and the weights are trained from scratch, there is no significant loss in test performance, i.e., \textbf{features are stored in the gates}. Further, gates of a trained network perform better than $\text{NTK}_{\infty}$ and gates from a untrained network performs poorly than $\text{NTK}_{\infty}$, i.e., \textbf{learning in the gates explains the difference between finite vs infinite} width DNNs with ReLUs.

$\bullet$ \textbf{Interpretable Kernel:} Each input has a corresponding \emph{active} sub-network comprising of the gates that are `on' and weights through such gates. This active sub-network is responsible for producing the output for that input. When the gates are decoupled from the weights, the NTK simplifies into a constant times a \emph{neural path kernel} (NPK) given by :
\begin{align}\label{eq:ntk-npk-relation}
\text{NTK}(x,x')\quad\propto\quad \text{NPK}(x,x')\quad =\quad \ip{x,x'}\cdot {\bf{overlap}}(x,x'),
\end{align}
where ${\bf{overlap}}(x,x')$ is a correlation matrix that measure the overlap of active sub-networks in terms of the  total number of paths that are `active' for both inputs $x$ and $x'$. 

\begin{comment}
\begin{center}
\emph{Duality: DNNs with ReLU are layers as well as paths}
\end{center}
The standard primal way of expressing information processing is layer by layer. In the dual view, the DNN is broken into paths. A path comprises of gates and weights, and a path is `active' or `on' only if all the gates in the path are `on'. The output is the sum of the contribution of the individual paths. 
\begin{center}
\emph{Most information (i.e., features) is in the gates and gates are learnt during training}
\end{center}
The gates are treated as masks and are decoupled from the weights by storing the gates and weights in two separate networks (see \Cref{sec:dgn}). Now, the information in the gates can be measured by fixing the gates and training the weights. It was shown that (i) when the gates from a trained DNN are used as masks, and the weights are trained from scratch, there is no significant loss in test performance, i.e., \textbf{features are stored in the gates} and (ii) gates of a trained network perform better than $\text{NTK}_{\infty}$ and gates from a untrained network performs poorly than $\text{NTK}_{\infty}$, i.e., \textbf{learning in the gates explains the difference between finite vs infinite} width DNNs with ReLUs.
\begin{center}
\emph{NTK is interpretable in terms of active sub-networks}
\end{center}
\end{comment}


\subsection{Our Contribution}
The dual view [\citenum{npk}] empirically showed that features are learnt in the gates and provided an analytical characterisation of the gates in terms of the NPK (\eqref{eq:ntk-npk-relation}. In this paper, we extend the dual view to provide newer insights about the gates.  We list the novel theoretical and empirical contributions in this paper. 

$\bullet$ \textbf{Theoretical:} We simplify \eqref{eq:ntk-npk-relation} in a manner that explicitises the role of gates, depth and width. In fully connected/dense case with depth `$d$' (number of layers) and width `$w$' (number of hidden units in each layer), we show that
\begin{align}\label{eq:main}
\text{NTK}(x,x') \propto \langle x,x'\rangle\cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w, 
 \end{align}
where $G_l(x)\in\{0,1\}^w$ is the binary feature encoding the gates of layer $l$. This provides the following hierarchical interpretation: the basic building units are the gates which form the binary feature $G_l(x)$ of that layer; width gives averaging (division by $w$) ; depth provides product structure $\Pi_{l=1}^{d-1} (\cdot)$.

$\bullet$ \textbf{Empirical:} We show the following interesting properties of the gates.
\begin{enumerate}
\item Information stored in the gates is robust to layers permutations.
\item Information stored in the gates is robust to  training and testing with a constant input. 
\item Information stored in the gates is robust to training first with random labels and then with true labels.
\item Adversarial examples can be transferred solely via the gates (even if the layers are permuted and the input is a constant).
\item Training only the gates achieves a test accuracy of $65\%$ on CIFAR-10. This demonstrates that the right sub-network (albeit consisting of random weights) can be indeed found with standard training.
\end{enumerate}

\textbf{Message.}  `What does a DNN with ReLUs learn?' - a lot can be answered by just looking at the gates and the expression in \eqref{eq:main} .

\textbf{Technical Extension.} In addition, we also extend the dual view to cover the cases of convolutions with pooling and skip connections. We show that convolutional layers with pooling makes the NPK \emph{rotationally invariant} (this property has already been noted to hold for the NTK). We also consider a residual network (ResNet) architecture with $(b+2)$ blocks of fully connected networks with `$b$' skip connections between them. Within this ResNet there are $i=1,\ldots,2^b$ possible fully connect networks, and then $\text{NPK}^{\text{residual}}=\sum_{i=1}^{2^b} C_i \text{NPK}^{\text{dense}}_i$, where $C_i>0$ are positive constants.

%\textbf{One-Line Summary:} The high level implication of our results is conceptual simplification (not necessarily algorithmic), i.e., in order to answer `what does a DNN with ReLUs learn?' one need not look beyond the expression $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. 




\textbf{Organisation.}

\textbf{Notation.}

We consider fully-connected DNNs with `$w$' hidden units per layer and `$d-1$' hidden layers. $\Theta\in\R^{\dnet}$ are the network weights, where $\dnet=\din w+(d-2)w^2+w$. The information flow is shown in \Cref{tb:basic}, where
$\Theta(i,j,l)$ is the weight connecting the $j^{th}$ hidden unit of layer $l-1$ to the $i^{th}$ hidden unit of layer $l$. Further, $\Theta(\cdot,\cdot,1)\in\R^{w\times \din}, \Theta(\cdot,\cdot,l)\in\R^{w\times w},\forall l\in\{2,\ldots,d-1\}, \Theta(\cdot,\cdot,d)\in\R^{1\times w}$.
\begin{table}[h]
\centering
\begin{tabular}{|l l lll|}\hline
Input Layer&: &$z_{x,\Theta}(0)$ &$=$ &$x$ \\
Pre-Activation Input&: & $q_{x,\Theta}(i,l)$& $=$ & $\sum_{j} \Theta(i,j,l)\cdot z_{x,\Theta}(j,l-1)$\\
Gating Values&: &$G_{x,\Theta}(i,l)$& $=$ & $\mathbbm{1}_{\{q_{x,\Theta}(i,l)>0\}}$\\
Hidden Layer Output&: &$z_{x,\Theta}(i,l)$ & $=$ & $q_{x,\Theta}(i,l)\cdot G_{x,\Theta}(i,l)$ \\
Final Output&: & $\hat{y}_{\Theta}(x)$ & $=$ & $\sum_{j\in[w]} \Theta(1,j,d-1)\cdot z_{x,\Theta}(j,d-1)$\\\hline
\end{tabular}
\caption{Here, $l\in[d-1],i\in[w]$, $j\in[\din]$ for $l=1$ and $j\in[w]$ for $l=2,\ldots,d-1$.} 
\label{tb:basic}
\end{table}

\begin{comment}

CNN, Resnet to say that the framework can be extended successfully.

Perhaps the second important and powerful insights lie in the experimental implication contribution

why do we even care for a specific theory?

All through we differentiate from \cite{npk}
\end{comment}