@misc{featvisualise,
  title = {Feature Visualization: How neural networks build up their understanding of images} ,
  author={Chris Olah and Alexander Mordvintsev and Ludwig Schubert},
    howpublished = {\url{https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis}},
      year= {2017}
}

@article{fcgp,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{convgp,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}

@misc{BenAli-1,
  title = {Reflections on Random Kitchen Sinks} ,
  author={Ali Rahimi and Ben Recht},
  howpublished = {\url{http://www.argmin.net/2017/12/05/kitchen-sinks/}},
      year={2017}
}


@misc{Lecun,
  title = {My take on Ali Rahimi's "Test of Time" award talk at NIPS} ,
  author={Yann Lecun},
    howpublished = {\url{https://www.facebook.com/yann.lecun/posts/10154938130592143}},
      year= {2017}
}


@misc{BenAli-2,
  title = {An Addendum to Alchemy} ,
  author={Ali Rahimi and Ben Recht},
  howpublished = {\url{http://www.argmin.net/2017/12/11/alchemy-addendum/}},
      year= {2017}
}


@misc{Aliresponse,
  title = {Response to {Y}ann {L}e{C}un} ,
  author={Ali Rahimi},
  howpublished = {\url{https://www2.isye.gatech.edu/~tzhao80/Ali_Response.pdf}},
        year= {2017}
}

@misc{Mickens,
  title = {Q: Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?
A: Because Keynote Speakers Make Bad Life Decisions and Are Poor Role Models} ,
  author={James Mickens},
  howpublished = {\url{https://www.youtube.com/watch?v=ajGX7odA87k}},
  year = {2018}
}


@inproceedings{depth1,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on learning theory},
  pages={907--940},
  year={2016}
}



@article{depth2,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}

@inproceedings{resnets,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{wide1,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{wide2,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}


@inproceedings{wide3,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2017}
}


@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8139--8148},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8570--8581},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10835--10845},
  year={2019}
}

@article{dln,
  title={A convergence theory for deep learning via over-parameterization},
 author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}


@article{ando,
  title={Majorization relations for Hadamard products},
  author={Ando, T},
  journal={Linear algebra and its applications},
  volume={223},
  pages={57--64},
  year={1995},
  publisher={Elsevier Science Publishing Company, Inc.}
}


@article{ben,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}



@article{galu,
  author    = {Jonathan Fiat and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {Decoupling Gating from Linearity},
  journal   = {CoRR},
  volume    = {abs/1906.05032},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05032},
  archivePrefix = {arXiv},
  eprint    = {1906.05032},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-05032},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{ganguli,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@inproceedings{shamir,
  title={Exponential Convergence Time of Gradient Descent for One-Dimensional Deep Linear Neural Networks},
  author={Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={2691--2713},
  year={2019}
}


@article{dudnn,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint, arXiv:1811.03804},
  year={2018}
}


@article{dudln,
  title={Width provably matters in optimization for deep linear neural networks},
  author={Du, Simon S and Hu, Wei},
  journal={arXiv preprint arXiv:1901.08572},
  year={2019}
}


@article{arora,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}


@inproceedings{ntk,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}


@article{lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@inproceedings{prune1,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}


@inproceedings{prune2,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  booktitle={Advances in neural information processing systems},
  pages={164--171},
  year={1993}
}

@article{prune3,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1135--1143},
  year={2015}
}

@article{prune4,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@article{randlabel,
  title={What Do Neural Networks Learn When Trained With Random Labels?},
  author={Maennel, Hartmut and Alabdulmohsin, Ibrahim and Tolstikhin, Ilya and Baldock, Robert JN and Bousquet, Olivier and Gelly, Sylvain and Keysers, Daniel},
  journal={arXiv preprint arXiv:2006.10455},
  year={2020}
}


@article{du2018,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{edgepop,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:1911.13299},
  year={2019}
}


@inproceedings{balestriero2018spline,
  title={A spline theory of deep learning},
  author={Balestriero, Randall and others},
  booktitle={International Conference on Machine Learning},
  pages={374--383},
  year={2018}
}



@article{srivastava2014understanding,
  title={Understanding locally competitive networks},
  author={Srivastava, Rupesh Kumar and Masci, Jonathan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1410.1165},
  year={2014}
}

@article{sss,
  author    = {Jonathan Fiat and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {Decoupling Gating from Linearity},
  journal   = {CoRR},
  volume    = {abs/1906.05032},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05032},
  archivePrefix = {arXiv},
  eprint    = {1906.05032},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-05032},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{jacot2019freeze,
  title={Freeze and chaos for dnns: an NTK view of batch normalization, checkerboard and boundary effects},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1907.05715},
  year={2019}
}

@inproceedings{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2015}
}


@misc{adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{davidnet,
    title={DavidNet},
    author={David C. Page},
    url = {https://github.com/davidcpage/cifar10-fast}
}

@article{npk,
  title={Neural Path Features and Neural Path Kernel: Understanding the role of gates in deep learning},
  author={Chandrashekar Lakshminarayanan and Amit Vikram Singh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}




@inproceedings{angle,
  title={Diverse neural network learns true target functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1216--1224},
  year={2017}
}


@article{mkl1,
  title={Multiple kernel learning algorithms},
  author={G{\"o}nen, Mehmet and Alpayd{\i}n, Ethem},
  journal={The Journal of Machine Learning Research},
  volume={12},
  pages={2211--2268},
  year={2011},
  publisher={JMLR. org}
}


@inproceedings{mkl2,
  title={Multiple kernel learning, conic duality, and the SMO algorithm},
  author={Bach, Francis R and Lanckriet, Gert RG and Jordan, Michael I},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={6},
  year={2004}
}


@article{mkl3,
  title={Large scale multiple kernel learning},
  author={Sonnenburg, S{\"o}ren and R{\"a}tsch, Gunnar and Sch{\"a}fer, Christin and Sch{\"o}lkopf, Bernhard},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={Jul},
  pages={1531--1565},
  year={2006}
}

@inproceedings{mkl4,
  title={Learning non-linear combinations of kernels},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  booktitle={Advances in neural information processing systems},
  pages={396--404},
  year={2009}
}


@inproceedings{disentangling,
  title={Disentangling Trainability and Generalization in Deep Neural Networks},
  author={Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel},
  booktitle={International Conference on Machine Learning},
  pages={10462--10472},
  year={2020},
  organization={PMLR}
}


@inproceedings{scaling,
  title={Towards a general theory of infinite-width limits of neural classifiers},
  author={Golikov, Eugene},
  booktitle={International Conference on Machine Learning},
  pages={3617--3626},
  year={2020},
  organization={PMLR}
}

@inproceedings{nth,
  title={Dynamics of deep neural networks and neural tangent hierarchy},
  author={Huang, Jiaoyang and Yau, Horng-Tzer},
  booktitle={International Conference on Machine Learning},
  pages={4542--4551},
  year={2020},
  organization={PMLR}
}



@inproceedings{ntkregression,
  title={The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization},
  author={Adlam, Ben and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={74--84},
  year={2020},
  organization={PMLR}
}


@inproceedings{meanres,
  title={A Mean Field Analysis Of Deep ResNet And Beyond: Towards Provably Optimization Via Overparameterization From Depth},
  author={Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
  booktitle={International Conference on Machine Learning},
  pages={6426--6436},
  year={2020},
  organization={PMLR}
}


@article{deepres,
  title={Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks?---A Neural Tangent Kernel Perspective},
  author={Huang, Kaixuan and Wang, Yuqing and Tao, Molei and Zhao, Tuo},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@article{spectra,
  title={Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks},
  author={Fan, Zhou and Wang, Zhichao},
  journal={arXiv preprint arXiv:2005.11879},
  year={2020}
}


@article{laplace,
  title={On the similarity between the laplace and neural tangent kernels},
  author={Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Basri, Ronen},
  journal={arXiv preprint arXiv:2007.01580},
  year={2020}
}



@article{label,
  title={Label-Aware Neural Tangent Kernel: Toward Better Generalization and Local Elasticity},
  author={Chen, Shuxiao and He, Hangfeng and Su, Weijie J},
  journal={arXiv preprint arXiv:2010.11775},
  year={2020}
}



@article{belkin,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}



@article{loss,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.15110},
  year={2020}
}


@article{genntk,
  title={A generalized neural tangent kernel analysis for two-layer neural networks},
  author={Chen, Zixiang and Cao, Yuan and Gu, Quanquan and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@article{subnet1,
  title={Interpreting and improving adversarial robustness of deep neural networks with neuron sensitivity},
  author={Zhang, Chongzhi and Liu, Aishan and Liu, Xianglong and Xu, Yitao and Yu, Hang and Ma, Yuqing and Li, Tianlin},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={1291--1304},
  year={2020},
  publisher={IEEE}
}