\begin{abstract}
Our aim is an improved and simplified understanding of the inner workings of deep neural network (DNNs) with rectified linear units (ReLUs). In particular, we focus on the gating property of ReLU, i.e., it either blocks (multiplies by $0$) or allows (multiplies by $1$) its pre-activation input. Due to the gating property, for each input there is an \emph{active/on} sub-network comprising those gates which are \emph{on} and the weights through those gates. Recently \cite{npk} showed that in DNNs with ReLUs most information is in the gates, and the gates are analytically characterised by a \emph{neural path kernel} (NPK) which, for a given pair of inputs is equal to the product of the inner product of the inputs and the size of the overlapping active sub-networks corresponding to the inputs.

In this paper, we `re-write' the NPK as a product of kernel, i.e., $\text{NPK}(x,x')= \ip{x,x'} \cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$, where $x,x'$ is a pair of inputs, $\ip{\cdot,\cdot}$ denotes the inner product, $w$ is width, $d$ is depth and $G_l(x)\in\{0,1\}^w$ is the binary feature vector which encodes the gates in layer $l$ for input $x$.  To our knowledge, this is the simplest kernel in literature that analytically characterises `what' is learnt in a DNN with ReLUs, and explicitises roles of depth, width and gates in a single expression  (we justify this empirically as well). We note that at the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, i.e, the \emph{correlation of active gates}. We show that operations such as (i) permuting the layers, (ii) arbitrarily tiling and rotation of the gates, (iii) providing a constant input do not degrade performance, because, in all these operations, the correlation of active gates is not lost. We also show via experiments that (open question related to) degradation in test accuracy due to upstream training with random label can be `root caused' to the gates. Based on our insights, we build two deep gated networks in which feature extraction is fully linear and is separate from the gating and the weights -- these achieve $75\%$ and $90\%$ of CIFAR-10.

%At the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, which measure the fraction of gates that are `on' for both inputs $x$ and $x'$ in each layer. Our simplified NPK expression explicitises the role of (i) activation is that of gating (ii) weights is to generate the gates, (iii) width is provide averaging in $\frac{\ip{G_l(x),G_l(x')}}w$ (iv) depth causes the product $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. 
\end{abstract}


\begin{comment}
\begin{abstract}
Rectified linear unit (ReLU) is also a gate which either blocks (multiplies by $0$) or allows (multiplies by $1$) its pre-activation input. Recently \cite{npk} showed that in deep neural networks (DNNs) with ReLU (i) most information is in the gates, (ii) learning in the gates during training explains why finite width DNNs outperform their corresponding infinite width \emph{neural tangent kernel} (NTK) (iii) gates can be decoupled from the weights to simplify the NTK into a \emph{neural path kernel} (NPK) which is only dependent on the input and the gates.

In this paper, we explicitise the role of gates, width and depth by showing that $\text{NPK}(x,x')= \ip{x,x'} \cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$, where $x,x'$ is a pair of inputs, $\ip{\cdot,\cdot}$ denotes the inner product, $w$ is width, $d$ is depth and $G_l(x)\in\{0,1\}^w$ is the binary feature vector which encodes the gates in layer $l$ for input $x$. We also show that in the presence of convolutions with pooling, the NPK is rotationally invariant, and in the presence of skip connections the NPK has a sum of product of base kernels structure. Based on our theory we setup novel experiments that uncover several perviously unknown properties of DNNs with ReLU $-$  the experiments add more evidence to the claims that \emph{most information is in the gates and gates are learnt}.
%At the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, which measure the fraction of gates that are `on' for both inputs $x$ and $x'$ in each layer. Our simplified NPK expression explicitises the role of (i) activation is that of gating (ii) weights is to generate the gates, (iii) width is provide averaging in $\frac{\ip{G_l(x),G_l(x')}}w$ (iv) depth causes the product $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. 
\end{abstract}
\end{comment}