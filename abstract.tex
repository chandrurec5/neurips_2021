\begin{abstract}
Our aim is an improved and simplified understanding of the inner workings of deep neural network (DNNs) with rectified linear units (ReLUs). In particular, we focus on the gating property of ReLU, i.e., it either blocks (multiplies by $0$) or allows (multiplies by $1$) its pre-activation input. Due to the gating property, for each input there is an \emph{active/on} sub-network comprising those gates which are \emph{on} and the weights between those gates. Recently \cite{npk} showed that in DNNs with ReLUs most information is in the gates, and the gates are analytically characterised by a \emph{neural path kernel} (NPK) which, for a given pair of inputs is equal to the product of the inner product of the inputs and the size of the overlapping active sub-networks corresponding to the inputs.

In this paper, we first show three results (one theoretical and two experimental) that further establish the importance of gates. The theoretical result is that each layer has a \emph{base kernel} measuring the \emph{correlation of gates}, and the NPK is a \emph{Hadamard} product of these \emph{base kernels} and the input Gram matrix. To our knowledge, this is the simplest kernel in literature that analytically characterises the gates. First experimental result (to justify the theoretical result) is that operations destroying the layer-by-layer structure such as permuting the layers, arbitrarily tiling and rotation of the gates and providing a constant input do not degrade performance, because, in all these operations, the correlation of active gates is not lost. Secondly we show experimentally that (open question related to) the degradation in test accuracy due to upstream training with random labels can be `root caused' to the gates -- this adds more strength to the dual view and the importance of gates. Finally, we modify standard architectures (VGG19 and a ResNet) to yield two deep gated networks in which feature extraction is free of activations and is separate from the gates and the weights -- these achieve greater than $90\%$ test accuracy on CIFAR-10.
\end{abstract}



\begin{comment}
\begin{abstract}
Our aim is an improved and simplified understanding of the inner workings of deep neural network (DNNs) with rectified linear units (ReLUs). In particular, we focus on the gating property of ReLU, i.e., it either blocks (multiplies by $0$) or allows (multiplies by $1$) its pre-activation input. Due to the gating property, for each input there is an \emph{active/on} sub-network comprising those gates which are \emph{on} and the weights between those gates. Recently \cite{npk} showed that in DNNs with ReLUs most information is in the gates, and the gates are analytically characterised by a \emph{neural path kernel} (NPK) which, for a given pair of inputs is equal to the product of the inner product of the inputs and the size of the overlapping active sub-networks corresponding to the inputs.

In this paper, we `re-write' the NPK as a product of kernel, i.e., $\text{NPK}(x,x')= \ip{x,x'} \cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$, where $x,x'$ is a pair of inputs, $\ip{\cdot,\cdot}$ denotes the inner product, $w$ is width, $d$ is depth and $G_l(x)\in\{0,1\}^w$ is the binary feature vector which encodes the gates in layer $l$ for input $x$.  To our knowledge, this is the simplest kernel in literature that analytically characterises `what is learnt in a DNN with ReLUs' (justified empirically as well), and explicitises roles of depth, width and gates in a single expression. We note that at the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, i.e, the \emph{correlation of active gates}. We show that operations destroying the layer-by-layer structure such as (i) permuting the layers, (ii) arbitrarily tiling and rotation of the gates, (iii) providing a constant input do not degrade performance, because, in all these operations, the correlation of active gates is not lost. We also show via experiments that (open question related to) degradation in test accuracy due to upstream training with random label can be `root caused' to the gates. 

%At the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, which measure the fraction of gates that are `on' for both inputs $x$ and $x'$ in each layer. Our simplified NPK expression explicitises the role of (i) activation is that of gating (ii) weights is to generate the gates, (iii) width is provide averaging in $\frac{\ip{G_l(x),G_l(x')}}w$ (iv) depth causes the product $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. 
\end{abstract}
\end{comment}

\begin{comment}
\begin{abstract}
Rectified linear unit (ReLU) is also a gate which either blocks (multiplies by $0$) or allows (multiplies by $1$) its pre-activation input. Recently \cite{npk} showed that in deep neural networks (DNNs) with ReLU (i) most information is in the gates, (ii) learning in the gates during training explains why finite width DNNs outperform their corresponding infinite width \emph{neural tangent kernel} (NTK) (iii) gates can be decoupled from the weights to simplify the NTK into a \emph{neural path kernel} (NPK) which is only dependent on the input and the gates.

In this paper, we explicitise the role of gates, width and depth by showing that $\text{NPK}(x,x')= \ip{x,x'} \cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$, where $x,x'$ is a pair of inputs, $\ip{\cdot,\cdot}$ denotes the inner product, $w$ is width, $d$ is depth and $G_l(x)\in\{0,1\}^w$ is the binary feature vector which encodes the gates in layer $l$ for input $x$. We also show that in the presence of convolutions with pooling, the NPK is rotationally invariant, and in the presence of skip connections the NPK has a sum of product of base kernels structure. Based on our theory we setup novel experiments that uncover several perviously unknown properties of DNNs with ReLU $-$  the experiments add more evidence to the claims that \emph{most information is in the gates and gates are learnt}.
%At the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, which measure the fraction of gates that are `on' for both inputs $x$ and $x'$ in each layer. Our simplified NPK expression explicitises the role of (i) activation is that of gating (ii) weights is to generate the gates, (iii) width is provide averaging in $\frac{\ip{G_l(x),G_l(x')}}w$ (iv) depth causes the product $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. 
\end{abstract}
\end{comment}