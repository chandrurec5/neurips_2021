\begin{abstract}
We study deep neural networks (DNNs) with rectified linear units (ReLUs). ReLUs are also gates (\emph{on/off}) which either blocks or allows its pre-activation input. Recent work of \cite{npk} developed a dual `neural-path' framework wherein a DNN with ReLU was broken down into paths, and the output was expressed as a summation of the contribution of individual paths. Using duality they showed that (i) most information is stored in the gates, (ii) gates are learnt during training and (iii) the \emph{neural tangent kernel} (NTK) simplifies into a \emph{neural path kernel} (NPK) which is solely dependent on the input and the gates.\\
In this paper, we extend the dual view to provide new theoretical and experiments results. Firstly, in the case of fully connected DNNs, we simplify the NPK by showing that $\text{NPK}(x,x')= \ip{x,x'} \cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$, where $x,x'$ is a pair of inputs, $w$ is width, $d$ is depth, $G_l(x)\in\{0,1\}^w$ is the binary feature vector which encodes the gates in layer $l$ for input $x$. At the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, which measure the fraction of gates that are `on' for both inputs $x$ and $x'$ in each layer. Our simplified NPK expression explicitises the role of (i) activation is that of gating (ii) weights is to generate the gates, (iii) width is provide averaging in $\frac{\ip{G_l(x),G_l(x')}}w$ (iv) depth causes the product $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. We also show that in the presence of convolutions with pooling, the NPK is rotationally invariant, and in the presence of skip connections the NPK has a sum of product of base kernels structure. Based on our theory we setup novel experiments to verify the claim that \emph{most information is stored in the gates of a DNN with ReLU}.
\end{abstract}

