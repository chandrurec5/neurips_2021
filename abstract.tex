\begin{abstract}
Rectified linear unit (ReLU) is also a gate which either blocks (multiplies by $0$) or allows (multiplies by $1$) its pre-activation input. Recently \cite{npk} showed that in deep neural networks (DNNs) with ReLU (i) most information is in the gates, (ii) learning in the gates during training explains why finite width DNNs outperform their corresponding infinite width \emph{neural tangent kernel} (NTK) (iii) gates can be decoupled from the weights to simplify the NTK into a \emph{neural path kernel} (NPK) which is only dependent on the input and the gates.

In this paper, we explicitise the role of gates, width and depth by showing that $\text{NPK}(x,x')= \ip{x,x'} \cdot \Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$, where $x,x'$ is a pair of inputs, $\ip{\cdot,\cdot}$ denotes the inner product, $w$ is width, $d$ is depth and $G_l(x)\in\{0,1\}^w$ is the binary feature vector which encodes the gates in layer $l$ for input $x$. We also show that in the presence of convolutions with pooling, the NPK is rotationally invariant, and in the presence of skip connections the NPK has a sum of product of base kernels structure. Based on our theory we setup novel experiments that uncover several perviously unknown properties of DNNs with ReLU $-$  the experiments add more evidence to the claims that \emph{most information is in the gates and gates are learnt}.
%At the heart are the \emph{base kernels} $\frac{\ip{G_l(x),G_l(x')}}w$, which measure the fraction of gates that are `on' for both inputs $x$ and $x'$ in each layer. Our simplified NPK expression explicitises the role of (i) activation is that of gating (ii) weights is to generate the gates, (iii) width is provide averaging in $\frac{\ip{G_l(x),G_l(x')}}w$ (iv) depth causes the product $\Pi_{l=1}^{d-1} \frac{\ip{G_l(x),G_l(x')}}w$. 
\end{abstract}

