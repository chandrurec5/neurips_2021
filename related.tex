\section{Other Related Works and Future Work}
We are primarily interested in a pedagogical nugget that helps us to interpret DNNs with ReLU. The main takeaway from duality \cite{npk} was most information is stored in the gates, which is characterised by the NPK. In this paper we further simplified the NPK to reveal for the first time literature additional properties such as invariance to layer permutations and constant inputs which were also verified experimentally. We have thus strengthened the view that gates are more important. Since learning in the gates takes the NPK/NTK significantly away from the randomised initialisation, a natural direction aligned with our work would be to understand how learning in the gates happens, as opposed to a design and analysis based on kernels at randomised initialisation \cite{disentangling,nth,meanres,deepres,spectra,laplace,belkin,label,fcgp,convgp}. The following are interesting future directions:

1)  Analysing learning in NPFs/gates seems to be an important problem. However, it is also a difficult problem to tackle in the case of DNNs with ReLU due to the fact that both NPFs and NPV are coupled. Our experiments showed that the DGN as a standalone network with learning of the gates (in DL regime) matches the performance of standard DNNs within $3\%$. Since in a DGN, the NTK$=\text{NTK}^{\text{fixed-gate}}+\text{NTK}^{\text{gate-learn}}$, it will be interesting (and perhaps easier)  to study $\text{NTK}^{\text{gate-learn}}$, which is related to learning in gates. 

2) Another direction is to look at the constant input experiment from an adversarial example standpoint: since the input to value network is a constant, the adversary can only attack via the feature network, i.e., the gates.  

3) It will also be interesting to investigate the practical benefits of the sum of product structure in ResNet.

\section{Conclusion}
In this paper, we proposed a \emph{pedagogical nugget} based on duality and neural tangent kernel, which provided a simplified understanding of deep neural networks with rectified linear units. Using this we explained/interpreted roles of activation, weights, depth, width, skip connections and convolutions with pooling in a simple manner. Based on our theory, we also showed empirically that permuting the layers or providing constant input does not degrade performance. The results (theoretical and empirical) strengthened the view that gates are more important. We concluded by pointing out some future directions, of which, understanding the learning in the gates is of primary importance.



\begin{comment}
Learning in gates is also takes away the importance of looking at NPK at randomised initialisation, a reason why we did not pursue the question of analysing the spectrum of NPK at randomised initialisation in the limit of infinite width/depth (like \cite{disentangling,spectra}) or design a pure kernel method (like \cite{arora2019,fcgp,convgp}) or its constancy as in \cite{belkin}. The NTK and NPFs are related at an algebraic level (see \Cref{prop:ntknew}), i.e., the relation holds for any width, depth, and initialisation, and not just in limiting case. Also, our result that convolutions with pooling make the NPFs rotationally invariant is again algebraic and holds for finite width/depth as opposed to an asymptotic analytical characterisation of pooling \cite{disentangle}. Similarly, the sum of product structure of ResNets is also an algebraic result as opposed to \cite{meanres} which shows that ResNets are an ensemble of shallow architecture by ignoring certain higher order terms in the mean-field analysis. \cite{loss} study the dynamics of NTK empirically and show that its performance matches that of full network training in $15\%$ to $45\%$ of training time. The fact that NPF/NPK/gates learning is continuous ($\text{NTK}^{\text{gate-learn}}$ dictates the dynamics of the gates) and that learnt NPF/NPK/gates perform as well as the original DNN was already empirically shown by \cite{npk}, and our experiments add strength for the same. 

Several recent works \cite{disentangling,nth,meanres,deepres,spectra,laplace,belkin,loss} have looked at the NTK. We are primarily interested in a pedagogical nugget that helps us to interpret DNNs with ReLU. Our work is based on duality \cite{npk} which differs at a conceptual level from the aforementioned works in the following ways: (i) firstly the role of ReLU is explicitly accounted by encoding them as NPFs, (ii) the connection between the NTK and NPF is algebraic (see \Cref{prop:ntknew}) and not just in the limiting case (iii) as per theory (\Cref{th:main}) so long as the weights of the value network are random and statistically decoupled from NPFs, the NPV do not play an important role and a fact which is also verified experimentally where using the NPFs alone the NPV could be trained from scratch (iv) the NPK can correspond to arbitrary finite width feature network weights (see remark on role of activations in \Cref{sec:fc}) and not just random.  Our experiments (as well as those by \cite{npk}) showing that learning in the gates is the difference between NTK and finite width DNNs is the key differentiator from \cite{nth,label} which also looked at difference between finite width DNNs and NTK. The learning in gates is also takes away the importance of looking at NPK at randomised initialisation, a reason why we did not pursue the question of analysing the spectrum of NPK at randomised initialisation in the limit of infinite width/depth (like \cite{disentangling,spectra}) or design a pure kernel method (like \cite{arora2019,fcgp,convgp}) or its constancy as in \cite{belkin}. Further, we believe that the NPK at randomised initialisation might also be associated with a simpler kernel in manner to the result that (\cite{laplace}) NTK for FC-DNN with ReLU  is closely related to the standard Laplace kernel. Our result that convolutions with pooling make the NPFs rotationally invariant is again algebraic and holds for finite width/depth as opposed to an asymptotic analytical characterisation of pooling \cite{disentangle}. Similarly, the sum of product structure of ResNets is also an algebraic result as opposed to \cite{meanres} which shows that ResNets are an ensemble of shallow architecture by ignoring certain higher order terms in the mean-field analysis. \cite{loss} study the dynamics of NTK empirically and show that its performance matches that of full network training in $15\%$ to $45\%$ of training time. The fact that NPF/NPK/gates learning is continuous ($\text{NTK}^{\text{gate-learn}}$ dictates the dynamics of the gates) and that learnt NPF/NPK/gates perform as well as the original DNN was already empirically shown by \cite{npk}, and our experiments add strength for the same. 


\cite{disentangling} (via a spectral analysis of the NTK) show presence of (i) ordered phase, in which the trainability of DNNs degrades at large depths, but their ability to generalise does not and (ii) chaotic phase, in which, trainability improves with depth, but generalisation degrades,  (iii)  pooling  improves the depth over which networks can generalise in the chaotic phase but reduces the depth in the ordered phase. \cite{spectra} show that the eigenvalue distributions of the Conjugate Kernel and Neural Tangent Kernel converge to deterministic limits. In order to explain the difference between the NTK and finite width DNNs, \cite{nth} derive an infinite hierarchy of differential equations known as the neural tangent hierarchy (NTH).  \cite{label} observe that the performance gap between NTK and finite width DNN may be be partly due to the label agnostic nature of the NTK and introduce a novel approach from the perspective of label-awareness to reduce this gap. \cite{meanres} use mean-field analyses of two-layer DNNs to propose several novel training schemes for ResNets that performs well on the standard datasets. \cite{deepres} compare the kernel of deep ResNets with that of deep FFNets and show that the class of functions induced by the kernel of i) FFNets degenerates asymptotically with depth and i) ResNets does not degenerate with depth. \cite{loss} study the dynamics of NTK and show that there is a highly chaotic rapid initial transient phase in which NTK changes rapidly, followed by a phase where the NTK changes at constant velocity, and its performance matches that of full network training in $15\%$ to $45\%$ of training time. \cite{genntk} provide a generalised NTK analysis and show that noisy gradient descent with weight decay can still exhibit a “kernel-like” behaviour. \cite{belkin} show that constancy of the NTK results from the scaling properties of the norm of the Hessian matrix of the network as a function of the network width. \

\textbf{Related to NTK:} \cite{disentangling} (via a spectral analysis of the NTK) show presence of (i) ordered phase, in which the trainability of DNNs degrades at large depths, but their ability to generalise does not and (ii) chaotic phase, in which, trainability improves with depth, but generalisation degrades,  (iii)  pooling  improves the depth over which networks can generalise in the chaotic phase but reduces the depth in the ordered phase. 
\cite{scaling}  propose a theory for infinite width DNNs that connects  mean-field (MF) and constant kernel (NTK) limits. \cite{ntkregression} analyse the high-dimensional asymptotic generalisation performance of kernel regression with the NTK of a single hidden-layer neural network.



 \textbf{Gates and Sub Networks:} \cite{srivastava2014understanding} analysed the role of gates empirically and via a t-SNE based analysis showed that ``subnetworks active for examples of the same class are much more similar to each other compared to the ones activated for the examples of different classes''. They also observe gates flip which is upto $20\%$ of examples in the initial phases of training but quickly settle down to $5\%$. \cite{subnet1}, study active sub-networks at sample level and class level to propose two adversarial example detection methods.

\textbf{Our Work:} In contrast to aforementioned works on NTK, the focus of this paper has been on the NPK which is based on the gates, and instead of a pure kernel method, we use the intuition obtained on the NPK to test a finite width DGN. Further, we believe that the dual view based interpretation is more direct (such as rotational invariance of NPK due to convolutions and pooling, a fact not noticed in prior work). Our empirical results are closely tied to the theory we develop which is absent in prior empirical works that analysed the role of gates and sub-networks.
\end{comment}

