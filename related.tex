\section{Related Works}
Two important kernels associated with a DNN are its neural tangent kernel (NTK) [\citenum{ntk,arora2019exact}], and the conjugate kernel (CK). The NTK is based on the correlation of the gradients, CK is based on the correlation of the network output. In contrast, the neural path kernel (NPK) in our paper is based on the correlation of the gates. Note that it is the NTK which simplifies into the NPK when the gates and the weights are decoupled. Infinite width NTK/CK at randomised initialisation has been examined analytically [\citenum{spectra,laplace,belkin,genntk,disentangling,label}] as well as empirically evaluated for its performance as a pure-kernel method [\citenum{convgp,fcgp,lee2020finite}]. In contrast, in this paper, we closely examine (empirically) the gates of a finite width DNN with ReLUs, for which the insights obtained from the NPK expression act as an aid.  An infinite hierarchy of differential equations called the neural tangent hierarchy (NTH) was proposed in [\citenum{nth}] to analytically capture the dynamics of the NTK in finite width networks. In contrast, in the dual view (our paper and in [\citenum{npk}]), the dynamics of the gates is captured by having the feature network to be either untrained, partially/fully trained. In [\citenum{loss}] the dynamics of the NTK is empirically characterised by a set of diverse measures, one of among which is to measure the hamming distance between the binary encodings of the ReLU activation patterns for different network weights. Using these measures it was shown that there is a highly chaotic rapid initial transient phase in which NTK changes rapidly, followed by a phase where the NTK changes at constant velocity, and its performance matches that of full network training in $15\%$ to $45\%$ of training time. In contrast, in the dual view, the activation pattern is formally coded in the neural path features, and the correlation between the activation patterns between two inputs (as opposed to weights) is captured by the NPK, and the dynamics of the gates is captured. In [\citenum{randlabel}], both positive and negative effects of upstream training with random labels was extensively studied analytically as well as empirically. However, the question was why the test performance degrades due to upstream training with random labels was left open, which we addressed in our paper. 
It was shown in [\citenum{li2019enhanced}], that  prediction using CNTK with GAP is equivalent to prediction using CNTK without GAP but with full translation data augmentation with wrap-around at the boundary. This is related to \Cref{th:mainconv}. It was shown in [\citenum{veit2016residual}] that residual networks behave like ensemble of shallow networks. This is related \Cref{th:mainres}. A spline theory based on max-affine linearity was proposed in [\citenum{balestriero2018spline}] to show that a DNN with ReLUs performs hierarchical, greedy template matching, and this framework was extended in [\citenum{balestriero2018hard}] to cover many non-linear activations. In contrast, the dual view exploits the gating property to simplify the NTK into the NPK.
Gated linearity was first explored in {sss} for single layered networks, along with a non-gradient algorithm to tune the gates. In contrast, we look at networks of any depth, and the gates are tuned in the decoupled learning' regime via standard optimisers. An important novelty in our work over all the aforementioned works is our DGN-No-Act where feature generation is activation free, explicit and not hidden.


%\texbf{ReLU.} It was shown in [\citenum{hanin2019deep}] that DNNs with ReLUs the average number of activation patterns at initialization is bounded by the total number of neurons raised to the input dimension, suggesting that the current methods are capable of realising full expressibility of deep networks.

\begin{comment}
Learning in gates is also takes away the importance of looking at NPK at randomised initialisation, a reason why we did not pursue the question of analysing the spectrum of NPK at randomised initialisation in the limit of infinite width/depth (like \cite{disentangling,spectra}) or design a pure kernel method (like \cite{arora2019,fcgp,convgp}) or its constancy as in \cite{belkin}. The NTK and NPFs are related at an algebraic level (see \Cref{prop:ntknew}), i.e., the relation holds for any width, depth, and initialisation, and not just in limiting case. Also, our result that convolutions with pooling make the NPFs rotationally invariant is again algebraic and holds for finite width/depth as opposed to an asymptotic analytical characterisation of pooling \cite{disentangle}. Similarly, the sum of product structure of ResNets is also an algebraic result as opposed to \cite{meanres} which shows that ResNets are an ensemble of shallow architecture by ignoring certain higher order terms in the mean-field analysis. \cite{loss} study the dynamics of NTK empirically and show that its performance matches that of full network training in $15\%$ to $45\%$ of training time. The fact that NPF/NPK/gates learning is continuous ($\text{NTK}^{\text{gate-learn}}$ dictates the dynamics of the gates) and that learnt NPF/NPK/gates perform as well as the original DNN was already empirically shown by \cite{npk}, and our experiments add strength for the same. 

Several recent works \cite{disentangling,nth,meanres,deepres,spectra,laplace,belkin,loss} have looked at the NTK. We are primarily interested in a pedagogical nugget that helps us to interpret DNNs with ReLU. Our work is based on duality \cite{npk} which differs at a conceptual level from the aforementioned works in the following ways: (i) firstly the role of ReLU is explicitly accounted by encoding them as NPFs, (ii) the connection between the NTK and NPF is algebraic (see \Cref{prop:ntknew}) and not just in the limiting case (iii) as per theory (\Cref{th:main}) so long as the weights of the value network are random and statistically decoupled from NPFs, the NPV do not play an important role and a fact which is also verified experimentally where using the NPFs alone the NPV could be trained from scratch (iv) the NPK can correspond to arbitrary finite width feature network weights (see remark on role of activations in \Cref{sec:fc}) and not just random.  Our experiments (as well as those by \cite{npk}) showing that learning in the gates is the difference between NTK and finite width DNNs is the key differentiator from \cite{nth,label} which also looked at difference between finite width DNNs and NTK. The learning in gates is also takes away the importance of looking at NPK at randomised initialisation, a reason why we did not pursue the question of analysing the spectrum of NPK at randomised initialisation in the limit of infinite width/depth (like \cite{disentangling,spectra}) or design a pure kernel method (like \cite{arora2019,fcgp,convgp}) or its constancy as in \cite{belkin}. Further, we believe that the NPK at randomised initialisation might also be associated with a simpler kernel in manner to the result that (\cite{laplace}) NTK for FC-DNN with ReLU  is closely related to the standard Laplace kernel. Our result that convolutions with pooling make the NPFs rotationally invariant is again algebraic and holds for finite width/depth as opposed to an asymptotic analytical characterisation of pooling \cite{disentangle}. Similarly, the sum of product structure of ResNets is also an algebraic result as opposed to \cite{meanres} which shows that ResNets are an ensemble of shallow architecture by ignoring certain higher order terms in the mean-field analysis. \cite{loss} study the dynamics of NTK empirically and show that its performance matches that of full network training in $15\%$ to $45\%$ of training time. The fact that NPF/NPK/gates learning is continuous ($\text{NTK}^{\text{gate-learn}}$ dictates the dynamics of the gates) and that learnt NPF/NPK/gates perform as well as the original DNN was already empirically shown by \cite{npk}, and our experiments add strength for the same. 


\cite{disentangling} (via a spectral analysis of the NTK) show presence of (i) ordered phase, in which the trainability of DNNs degrades at large depths, but their ability to generalise does not and (ii) chaotic phase, in which, trainability improves with depth, but generalisation degrades,  (iii)  pooling  improves the depth over which networks can generalise in the chaotic phase but reduces the depth in the ordered phase. \cite{spectra} show that the eigenvalue distributions of the Conjugate Kernel and Neural Tangent Kernel converge to deterministic limits. In order to explain the difference between the NTK and finite width DNNs, \cite{nth} derive an infinite hierarchy of differential equations known as the neural tangent hierarchy (NTH).  \cite{label} observe that the performance gap between NTK and finite width DNN may be be partly due to the label agnostic nature of the NTK and introduce a novel approach from the perspective of label-awareness to reduce this gap. \cite{meanres} use mean-field analyses of two-layer DNNs to propose several novel training schemes for ResNets that performs well on the standard datasets. \cite{deepres} compare the kernel of deep ResNets with that of deep FFNets and show that the class of functions induced by the kernel of i) FFNets degenerates asymptotically with depth and i) ResNets does not degenerate with depth. \cite{loss} study the dynamics of NTK and show that there is a highly chaotic rapid initial transient phase in which NTK changes rapidly, followed by a phase where the NTK changes at constant velocity, and its performance matches that of full network training in $15\%$ to $45\%$ of training time. \cite{genntk} provide a generalised NTK analysis and show that noisy gradient descent with weight decay can still exhibit a “kernel-like” behaviour. \cite{belkin} show that constancy of the NTK results from the scaling properties of the norm of the Hessian matrix of the network as a function of the network width. \

\textbf{Related to NTK:} \cite{disentangling} (via a spectral analysis of the NTK) show presence of (i) ordered phase, in which the trainability of DNNs degrades at large depths, but their ability to generalise does not and (ii) chaotic phase, in which, trainability improves with depth, but generalisation degrades,  (iii)  pooling  improves the depth over which networks can generalise in the chaotic phase but reduces the depth in the ordered phase. 
\cite{scaling}  propose a theory for infinite width DNNs that connects  mean-field (MF) and constant kernel (NTK) limits. \cite{ntkregression} analyse the high-dimensional asymptotic generalisation performance of kernel regression with the NTK of a single hidden-layer neural network.



 \textbf{Gates and Sub Networks:} \cite{srivastava2014understanding} analysed the role of gates empirically and via a t-SNE based analysis showed that ``subnetworks active for examples of the same class are much more similar to each other compared to the ones activated for the examples of different classes''. They also observe gates flip which is upto $20\%$ of examples in the initial phases of training but quickly settle down to $5\%$. \cite{subnet1}, study active sub-networks at sample level and class level to propose two adversarial example detection methods.

\textbf{Our Work:} In contrast to aforementioned works on NTK, the focus of this paper has been on the NPK which is based on the gates, and instead of a pure kernel method, we use the intuition obtained on the NPK to test a finite width DGN. Further, we believe that the dual view based interpretation is more direct (such as rotational invariance of NPK due to convolutions and pooling, a fact not noticed in prior work). Our empirical results are closely tied to the theory we develop which is absent in prior empirical works that analysed the role of gates and sub-networks.
\end{comment}

