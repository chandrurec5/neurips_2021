\section{Conclusion}
We showed that the correlation of the gates captures `what is learnt by a DNN with ReLUs', and  that operations that destroy layer-by-layer structure do not degrade performance as long as the correlation of the gates is not lost. We also showed that upstream training with random labels affects the gates which is the reason for degradation of test performance even after down stream training with true labels. We also proposed a deep gated network in which feature extraction was free from activations, therefore, explicit and not hidden, and achieved greater than $90\%$ test accuracy on CIFAR-10.
